import argparse

import numpy as np
from dgld.models.pygod.detector import DMGD
from sklearn.metrics import average_precision_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_auc_score
from the_utils import save_to_csv_files
from the_utils import set_device
from the_utils import set_seed
from the_utils import tab_printer
from torch_geometric.utils import from_dgl

# pylint: disable=pointless-string-statement
"""
    Parameters
    ----------
    hid_dim :  int, optional
        Hidden dimension of model. Default: ``64``.
    num_layers : int, optional
        Total number of layers in model. Default: ``2``.
    dropout : float, optional
        Dropout rate. Default: ``0.``.
    weight_decay : float, optional
        Weight decay (L2 penalty). Default: ``0.``.
    act : callable activation function or None, optional
        Activation function if not None.
        Default: ``torch.nn.functional.relu``.
    backbone : torch.nn.Module
        The backbone of the deep detector implemented in PyG.
        Default: ``torch_geometric.nn.GCN``.
    contamination : float, optional
        The amount of contamination of the dataset in (0., 0.5], i.e.,
        the proportion of outliers in the dataset. Used when fitting to
        define the threshold on the decision function. Default: ``0.1``.
    lr : float, optional
        Learning rate. Default: ``0.004``.
    epoch : int, optional
        Maximum number of training epoch. Default: ``100``.
    gpu : int
        GPU Index, -1 for using CPU. Default: ``-1``.
    batch_size : int, optional
        Minibatch size, 0 for full batch training. Default: ``0``.
    num_neigh : int, optional
        Number of neighbors in sampling, -1 for all neighbors.
        Default: ``-1``.
    alpha : float, optional
        Weight of the radius loss. Default: ``1``.
    beta : float, optional
        Weight of the reconstruction loss. Default: ``1``.
    gamma : float, optional
        Weight of the homophily loss. Default: ``1``.
    warmup : int, optional
        The number of epochs for warm-up training. Default: ``2``.
    k : int, optional
        The number of clusters. Default: ``2``.
    verbose : int, optional
        Verbosity mode. Range in [0, 3]. Larger value for printing out
        more log information. Default: ``0``.
    save_emb : bool, optional
        Whether to save the embedding. Default: ``False``.
    compile_model : bool, optional
        Whether to compile the model with ``torch_geometric.compile``.
        Default: ``False``.
    **kwargs
        Other parameters for the backbone model.

    Attributes
    ----------
    decision_score_ : torch.Tensor
        The outlier scores of the training data. Outliers tend to have
        higher scores. This value is available once the detector is
        fitted.
    threshold_ : float
        The threshold is based on ``contamination``. It is the
        :math:`N \\times` ``contamination`` most abnormal samples in
        ``decision_score_``. The threshold is calculated for generating
        binary outlier labels.
    label_ : torch.Tensor
        The binary labels of the training data. 0 stands for inliers
        and 1 for outliers. It is generated by applying
        ``threshold_`` on ``decision_score_``.
    emb : torch.Tensor or tuple of torch.Tensor or None
        The learned node hidden embeddings of shape
        :math:`N \\times` ``hid_dim``. Only available when ``save_emb``
        is ``True``. When the detector has not been fitted, ``emb`` is
        ``None``. When the detector has multiple embeddings,
        ``emb`` is a tuple of torch.Tensor.
    """

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DMGD")
    parser.add_argument("--dataset", type=str, default="Amazon")
    parser.add_argument("--gpu", type=str, default="0", help="GPU id")
    # parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-5)
    parser.add_argument("--hid_dim", type=int, default=128)
    parser.add_argument("--lr", type=float, default=0.0001)
    parser.add_argument("--dropout", type=float, default=0.5)
    parser.add_argument("--num_layers", type=int, default=5)
    parser.add_argument("--beta", type=float, default=0.5)
    parser.add_argument("--k", type=int, default=2)
    parser.add_argument("--nni", action="store_true", default=False)

    args = parser.parse_args()
    try:
        import nni

        NNI = args.nni
    except Exception as e:
        print("no NNI")
        NNI = False

    params = {
        "hid_dim": 64,
        "lr": 0.001,
        "dropout": 0.2,
        "num_layers": 1,
        "beta": 0.5,
        "k": 2,
        **args.__dict__,
    }
    optimized_params = nni.get_next_parameter() if NNI else {}
    params.update(optimized_params)

    device = set_device(args.gpu)

    from dgld.utils.load_data import (
        load_data,
        load_custom_data,
        load_truth_data,
        load_local,
    )
    from dgld.utils.inject_anomalies import (
        inject_contextual_anomalies,
        inject_structural_anomalies,
    )
    from dgld.utils.common_params import Q_MAP, K, P
    import torch

    data_name = args.dataset
    truth_list = [
        "weibo",
        "tfinance",
        "tsocial",
        "reddit",
        "Amazon",
        "Class",
        "Disney",
        "elliptic",
        "Enron",
        "wiki",
        "Facebook",
        "YelpChi",
    ]
    data_path = "dgld/data"
    if data_name in truth_list:
        # graph = load_truth_data(data_path=data_path,dataset_name=data_name)
        if data_name in ["Facebook", "YelpChi"]:
            graph = load_local(data_name=data_name, raw_dir=data_path)
        else:
            graph = load_truth_data(data_path=data_path,
                                    dataset_name=data_name)
    elif data_name == "custom":
        graph = load_custom_data(data_path=data_path)
    else:
        graph = load_data(data_name)
        graph = inject_contextual_anomalies(graph=graph,
                                            k=K,
                                            p=P,
                                            q=Q_MAP[data_name],
                                            seed=4096)
        graph = inject_structural_anomalies(graph=graph,
                                            p=P,
                                            q=Q_MAP[data_name],
                                            seed=4096)
        if data_name not in ["BlogCatalog", "Flickr"]:
            from sklearn.preprocessing import LabelEncoder, MinMaxScaler

            mms = MinMaxScaler(feature_range=(0, 1))
            tmp_feat = mms.fit_transform(graph.ndata["feat"].numpy())
            graph.ndata["feat"] = torch.from_numpy(np.array(tmp_feat))

    data = from_dgl(graph)
    data.name = data_name
    data.num_classes = 2
    data.x = data.feat
    data.y = data.label
    data.num_nodes = graph.num_nodes()
    data.num_edges = graph.num_edges()
    data.edge_index = torch.stack(graph.edges(), dim=0)
    data = data

    tab_printer(params)

    runs = 3
    import random

    seed_list = [random.randint(0, 9999999) for i in range(runs)]
    aucs = []
    for run in range(runs):
        set_seed(seed_list[run])
        model = DMGD(
            hid_dim=params["hid_dim"],
            num_layers=params["num_layers"],
            beta=params["beta"],
            lr=params["lr"],
            epoch=100,
            dropout=params["dropout"],
            weight_decay=params["weight_decay"],
            gpu=params["gpu"],
            k=params["k"],
            batch_size=params["batch_size"],
            num_neigh=-1 if params["batch_size"] == 0 else 3,
            verbose=2,
        )
        print(model.__class__.__name__)

        model.fit(data=data, label=data.y.cpu(), NNI=NNI)
        # get outlier scores on the training data (transductive setting)
        score = model.decision_score_

        # predict labels and scores on the testing data (inductive setting)
        # pred, score = model.predict(data, return_score=True)

        from dgld.utils.evaluation import split_auc

        auc, a_score, s_score = split_auc(data.y.cpu(), score)
        aucs.append(auc)
    if NNI:
        nni.report_final_result(np.array(aucs).mean())
    from the_utils import save_to_csv_files

    save_path = "baselines.csv"
    print(f"write result to {save_path}")
    save_to_csv_files(
        results={
            "AUC":
            f"{np.array(aucs).mean()*100:.2f}Â±{np.array(aucs).std()*100:.2f}",
        },
        insert_info={
            "dataset": args.dataset,
            "model": model.__class__.__name__,
        },
        append_info={"args": params},
        csv_name=save_path,
    )

    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_flickr.yaml --port 8881 > logs/gadnr_flickr.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_blog.yaml --port 8882 > logs/gadnr_blog.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_red.yaml --port 8884 > logs/gadnr_red.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_wiki.yaml --port 8885 > logs/gadnr_wiki.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_enr.yaml --port 8886 > logs/gadnr_enr.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_fb.yaml --port 8887 > logs/gadnr_fb.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_ama.yaml --port 8888 > logs/gadnr_ama.log &
    # nohup nnictl create --config dgld/models/pygod/train/gadnr/gadnr_yelp.yaml --port 8889 > logs/gadnr_yelp.log &
    # NOTE: `.env/lib/python3.8/site-packages/nni/experiment/launcher.py` modify `_check_rest_server(port, url_prefix=url_prefix)` into `_check_rest_server(port, url_prefix=url_prefix, retry=20)`
